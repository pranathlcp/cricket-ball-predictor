# Optimal Lambda when Classification Accuracy is used as the Evaluation Metric
# optimal_lambda_accuracy_lasso_w = 0.037
optimal_lambda1_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "accuracy"])]
optimal_lambda1_lasso_w
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_lambda_avg_class_accuracy_lasso_w = 0
optimal_lambda2_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "avg_class_accuracy"])]
optimal_lambda2_lasso_w
rf_eval_metrics_nw = read.csv("forest_validation_results_nw.csv")
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
m_values_nw = seq(from = 2, to = 19, by = 1)
n_m_values_nw = length(m_values_nw)
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5, type = 'b')
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
#
max_val_accuracy_rf_nw = max(rf_eval_metrics_nw[, "accuracy"])
max_val_accuracy_rf_nw
#
max_val_avg_class_accuracy_rf_nw = max(rf_eval_metrics_nw[, "avg_class_accuracy"])
max_val_avg_class_accuracy_rf_nw
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_nw = 2
optimal_m1_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "accuracy"])]
optimal_m1_rf_nw
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_m2_rf_nw = 2
optimal_m2_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "avg_class_accuracy"])]
optimal_m2_rf_nw
m_values_w = seq(from = 2, to = 19, by = 1)
n_m_values_w = length(m_values_w)
# write.csv(rf_eval_metrics_w, "rf_eval_metrics_w.csv", row.names = FALSE)
rf_eval_metrics_w = read.csv("forest_validation_results_w.csv")
plot(m_values_w, rf_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.70), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
plot(m_values_w, rf_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.40), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
plot(m_values_w, rf_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.40), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
plot(m_values_w, rf_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0.20, 0.40), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_rf_w =
max_val_accuracy_rf_w = max(rf_eval_metrics_w[, "accuracy"])
max_val_accuracy_rf_w
# max_val_avg_class_accuracy_rf_w =
max_val_avg_class_accuracy_rf_w = max(rf_eval_metrics_w[, "avg_class_accuracy"])
max_val_avg_class_accuracy_rf_w
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_w =
optimal_m1_rf_w = m_values_w[which.max(rf_eval_metrics_w[, "accuracy"])]
optimal_m1_rf_w
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_m2_rf_w = 0
optimal_m2_rf_w = m_values_w[which.max(rf_eval_metrics_w[, "avg_class_accuracy"])]
optimal_m2_rf_w
optimal_m2_rf_w
require(glmnet)
require(randomForest)
# A function for computing the the following metrics for classification problems
# 1) Classification Accuracy
# 2) Average Class Accuracy
# 3) F1 - Score (Macro Averaged)
compute_eval_metrics = function(predicted, actual) {
# Input:
#  predicted: A vector containing the predicted classes
#  actual: A vector containing the truth values
#
# Output:
#  A vector with "Classification Accuracy", "Average Class Accuracy", and
#  "F1 - Score" respectively
#
predicted <- factor(as.character(predicted), levels=sort(unique(as.character(actual))))
actual  <- as.factor(actual)
confusion_matrix = as.matrix(table(actual, predicted))
precision = diag(confusion_matrix) / colSums(confusion_matrix)
recall = diag(confusion_matrix) / rowSums(confusion_matrix)
## Computing the Classification Accuracy
accuracy = mean(predicted == actual)
## Computing the Average Class Accuracy (getting the arithmetic mean)
average_class_accuracy = mean(recall)
## Computing the F1 - Score (Macro Average)
f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
# Assuming that F1 is zero when it's not possible compute it
f1[is.na(f1)] <- 0
macro_f1 = mean(f1)
metric_list = c(accuracy, average_class_accuracy, macro_f1)
return(metric_list)
}
# The following columns are the actual columns used for modelling
# Other columns from the dataset are metadata relevant to each event
columns_for_modeling = c("outcome", "innings_score_pred", "innings_wickets_pred",
"innings_legal_balls_pred", "innings_run_rate_pred",
"striker_cum_avg_pred", "striker_cum_sr_pred",
"striker_avg_bowler_style_pred", "striker_sr_bowler_style_pred",
"striker_cum_four_rate_pred", "striker_cum_sixes_rate_pred",
"last_ball_score", "striker_current_score_pred",
"striker_current_sr_pred", "balls_without_boundary_pred",
"rrr_pred", "bowler_avg_striker_style_pred",
"bowler_sr_striker_style_pred", "bowler_cum_avg_pred",
"bowler_cum_sr_pred")
# Loading Training Data and considering only the relevant columns for modelling
training_data = read.csv("cricket_training.csv")
training_data = training_data[, columns_for_modeling]
n_training = dim(training_data)[1]
table(training_data$outcome)*100/n_training
# Converting the outcome variable to factors
training_data$outcome = factor(training_data$outcome)
trainingX = model.matrix(outcome ~ ., data = training_data)
trainingY = training_data$outcome
# Computing Weights
training_weights = as.vector(1 / (table(trainingY)[trainingY] / length(trainingY)))
# Loading Validation Data and considering only the relevant columns for modelling
validation_data = read.csv("cricket_validation.csv")
validation_data = validation_data[, columns_for_modeling]
n_validation = dim(validation_data)[1]
# Converting the outcome variable to factors
validation_data$outcome = factor(validation_data$outcome)
validationX = model.matrix(outcome ~ ., data = validation_data)
validationY = validation_data$outcome
# Loading Testing Data and considering only the relevant columns for modelling
testing_data = read.csv("cricket_testing.csv")
testing_data = testing_data[, columns_for_modeling]
n_testing = dim(testing_data)[1]
# Converting the outcome variable to factors
testing_data$outcome = factor(testing_data$outcome)
testingX = model.matrix(outcome ~ ., data = testing_data)
testingY = testing_data$outcome
final_training_data = rbind(training_data, validation_data)
final_trainingX = model.matrix(outcome ~ ., data = final_training_data)
final_trainingY = final_training_data$outcome
# Computing Weights
final_training_weights = as.vector(1 / (table(final_trainingY)[final_trainingY] / length(final_trainingY)))
lambda_values_nw = seq(from = 0, to = 0.12, by = 0.001)
n_lambda_values_nw = length(lambda_values_nw)
lasso_eval_metrics_nw = setNames(data.frame(matrix(ncol = 4, nrow = n_lambda_values_nw)),
c("lambda", "accuracy", "avg_class_accuracy", "f1"))
lasso_eval_metrics_nw[, "lambda"] = lambda_values_nw
for(i in 1:n_lambda_values_nw){
lambda = lambda_values_nw[i]
fit = glmnet(trainingX, trainingY, family = 'multinomial', lambda = lambda)
validation_yHat = predict(fit, newx = validationX, type='class')
validation_yHat = factor(validation_yHat, levels = levels(validationY))
eval_metrics = compute_eval_metrics(validation_yHat, validationY)
lasso_eval_metrics_nw[i, "accuracy"] = eval_metrics[1]
lasso_eval_metrics_nw[i, "avg_class_accuracy"] = eval_metrics[2]
lasso_eval_metrics_nw[i, "f1"] = eval_metrics[3]
print(paste0("Iteration ", i, " out of ", n_lambda_values_nw))
}
plot(lambda_values_nw, lasso_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.70), xlab = "Lambda", ylab = "Evaluation Metric Value",
main = "Lambda vs Evaluation Metric (Non - Weighted Logistic Regression)",
pch = 20, lwd = 0.5)
points(lambda_values_nw, lasso_eval_metrics_nw[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(lambda_values_nw, lasso_eval_metrics_nw[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_lasso_nw = 0.4394936
max_val_accuracy_lasso_nw = max(lasso_eval_metrics_nw[, "accuracy"])
max_val_accuracy_lasso_nw
# max_val_avg_class_accuracy_lasso_nw = 0.2014848
max_val_avg_class_accuracy_lasso_nw = max(lasso_eval_metrics_nw[, "avg_class_accuracy"])
max_val_avg_class_accuracy_lasso_nw
# Optimal Lambda when Classification Accuracy is used as the Evaluation Metric
# optimal_lambda_accuracy_lasso_nw = 0.071
optimal_lambda1_lasso_nw = lambda_values_nw[which.max(lasso_eval_metrics_nw[, "accuracy"])]
optimal_lambda1_lasso_nw
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_lambda_avg_class_accuracy_lasso_nw = 0
optimal_lambda2_lasso_nw = lambda_values_nw[which.max(lasso_eval_metrics_nw[, "avg_class_accuracy"])]
optimal_lambda2_lasso_nw
# Applying the Optimal Lambda 1 to Test Dataset (Accuracy as Metric)
best_fit1_lasso_nw = glmnet(final_trainingX, final_trainingY, family = 'multinomial', lambda = optimal_lambda1_lasso_nw)
testing_yHat1_lasso_nw = predict(best_fit1_lasso_nw, newx = testingX, type = "class")
testing_yHat1_lasso_nw = factor(testing_yHat1_lasso_nw, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat_lasso)
# Applying the Optimal Lambda 1 to Test Dataset (Accuracy as Metric)
best_fit1_lasso_nw = glmnet(final_trainingX, final_trainingY, family = 'multinomial', lambda = optimal_lambda1_lasso_nw)
testing_yHat1_lasso_nw = predict(best_fit1_lasso_nw, newx = testingX, type = "class")
testing_yHat1_lasso_nw = factor(testing_yHat1_lasso_nw, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat1_lasso_nw)
# Results
testing_accuracy1_lasso_nw = compute_eval_metrics(testing_yHat1_lasso_nw, testingY)[1]
testing_avg_class_accuracy1_lasso_nw = compute_eval_metrics(testing_yHat1_lasso_nw, testingY)[2]
testing_f1_score1_lasso_nw = compute_eval_metrics(testing_yHat1_lasso_nw, testingY)[3]
# Results
testing_accuracy1_lasso_nw = compute_eval_metrics(testing_yHat1_lasso_nw, testingY)[1]
testing_accuracy1_lasso_nw
testing_avg_class_accuracy1_lasso_nw
testing_f1_score1_lasso_nw
lambda_values_w = seq(from = 0, to = 0.12, by = 0.001)
n_lambda_values_w = length(lambda_values_w)
lasso_eval_metrics_w = setNames(data.frame(matrix(ncol = 4, nrow = n_lambda_values_w)),
c("lambda", "accuracy", "avg_class_accuracy", "f1"))
lasso_eval_metrics_w[, "lambda"] = lambda_values_w
# write.csv(lasso_eval_metrics_w, "lasso_validation_results_w.csv", row.names = FALSE)
lasso_eval_metrics_w = read.csv("lasso_validation_results_w.csv")
plot(lambda_values_w, lasso_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.70), xlab = "Lambda", ylab = "Evaluation Metric Value",
main = "Lambda vs Evaluation Metric (Non - Weighted Logistic Regression)",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
plot(lambda_values_w, lasso_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.40), xlab = "Lambda", ylab = "Evaluation Metric Value",
main = "Lambda vs Evaluation Metric (Non - Weighted Logistic Regression)",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_lasso_w = 0.3232785
max_val_accuracy_lasso_w = max(lasso_eval_metrics_w[, "accuracy"])
max_val_accuracy_lasso_w
# max_val_accuracy_lasso_w = 0.3232785
max_val_accuracy_lasso_w = max(lasso_eval_metrics_w[, "accuracy"])
max_val_accuracy_lasso_w
# max_val_avg_class_accuracy_lasso_w = 0.2439373
max_val_avg_class_accuracy_lasso_w = max(lasso_eval_metrics_w[, "avg_class_accuracy"])
max_val_avg_class_accuracy_lasso_w
# Optimal Lambda when Classification Accuracy is used as the Evaluation Metric
# optimal_lambda_accuracy_lasso_w = 0.037
optimal_lambda1_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "accuracy"])]
optimal_lambda1_lasso_w
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_lambda_avg_class_accuracy_lasso_w = 0
optimal_lambda2_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "avg_class_accuracy"])]
optimal_lambda2_lasso_w
lambda_values_w = seq(from = 0, to = 0.12, by = 0.001)
n_lambda_values_w = length(lambda_values_w)
lasso_eval_metrics_w = setNames(data.frame(matrix(ncol = 4, nrow = n_lambda_values_w)),
c("lambda", "accuracy", "avg_class_accuracy", "f1"))
lasso_eval_metrics_w[, "lambda"] = lambda_values_w
for(i in 1:n_lambda_values_w){
lambda = lambda_values_w[i]
fit = glmnet(trainingX, trainingY, family = 'multinomial', lambda = lambda,
weights = training_weights)
validation_yHat = predict(fit, newx = validationX, type='class')
validation_yHat = factor(validation_yHat, levels = levels(validationY))
eval_metrics = compute_eval_metrics(validation_yHat, validationY)
lasso_eval_metrics_w[i, "accuracy"] = eval_metrics[1]
lasso_eval_metrics_w[i, "avg_class_accuracy"] = eval_metrics[2]
lasso_eval_metrics_w[i, "f1"] = eval_metrics[3]
print(paste0("Iteration ", i, " out of ", n_lambda_values_w))
}
plot(lambda_values_w, lasso_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0, 0.40), xlab = "Lambda", ylab = "Evaluation Metric Value",
main = "Lambda vs Evaluation Metric (Non - Weighted Logistic Regression)",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(lambda_values_w, lasso_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_lasso_w = 0.3232785
max_val_accuracy_lasso_w = max(lasso_eval_metrics_w[, "accuracy"])
max_val_accuracy_lasso_w
# max_val_avg_class_accuracy_lasso_w = 0.2439373
max_val_avg_class_accuracy_lasso_w = max(lasso_eval_metrics_w[, "avg_class_accuracy"])
max_val_avg_class_accuracy_lasso_w
# Optimal Lambda when Classification Accuracy is used as the Evaluation Metric
# optimal_lambda_accuracy_lasso_w = 0.037
optimal_lambda1_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "accuracy"])]
optimal_lambda1_lasso_w
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_lambda_avg_class_accuracy_lasso_w = 0
optimal_lambda2_lasso_w = lambda_values_w[which.max(lasso_eval_metrics_w[, "avg_class_accuracy"])]
optimal_lambda2_lasso_w
# Applying the Optimal Lambda 1 to Test Dataset (Accuracy as Metric)
best_fit1_lasso_w = glmnet(final_trainingX, final_trainingY, family = 'multinomial',
lambda = optimal_lambda1_lasso_w, weights = final_training_weights)
testing_yHat1_lasso_w = predict(best_fit1_lasso_w, newx = testingX, type = "class")
testing_yHat1_lasso_w = factor(testing_yHat1_lasso_w, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat1_lasso_w)
# Results
testing_accuracy1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[1]
testing_accuracy1_lasso_w
testing_avg_class_accuracy1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[2]
testing_avg_class_accuracy1_lasso_w
testing_f1_score1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[3]
testing_f1_score1_lasso_w
optimal_lambda2_lasso_w
## Applying the Optimal Lambda 2 to Test Dataset (Average Class Accuracy as Evaluation Metric)
best_fit2_lasso_w = glmnet(final_trainingX, final_trainingY, family = 'multinomial',
lambda = optimal_lambda2_lasso_w, weights = final_training_weights)
testing_yHat2_lasso_w = predict(best_fit2_lasso_w, newx = testingX, type = "class")
testing_yHat2_lasso_w = factor(testing_yHat2_lasso_w, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat2_lasso_w)
testing_accuracy2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[1]
testing_accuracy2_lasso_w
# testing_avg_class_accuracy2_lasso_w =
testing_avg_class_accuracy2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[2]
testing_avg_class_accuracy2_lasso_w
# testing_f1_score2_lasso_w =
testing_f1_score2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[3]
testing_f1_score2_lasso_w
testing_f1_score2_lasso_w
n_trees = 500
m_values_nw = seq(from = 2, to = 19, by = 1)
n_m_values_nw = length(m_values_nw)
rf_eval_metrics_nw = setNames(data.frame(matrix(ncol = 4, nrow = n_m_values_nw)),
c("m", "accuracy", "avg_class_accuracy", "f1"))
rf_eval_metrics_nw[, "m"] = m_values_nw
rf_eval_metrics_nw = read.csv("forest_validation_results_nw.csv")
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
# max_val_accuracy_rf_nw = 0.4167037
max_val_accuracy_rf_nw = max(rf_eval_metrics_nw[, "accuracy"])
max_val_accuracy_rf_nw
# max_val_avg_class_accuracy_rf_nw = 0.1969875
max_val_avg_class_accuracy_rf_nw = max(rf_eval_metrics_nw[, "avg_class_accuracy"])
max_val_avg_class_accuracy_rf_nw
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_nw = 2
optimal_m1_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "accuracy"])]
optimal_m1_rf_nw
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_nw = 2
optimal_m1_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "accuracy"])]
optimal_m1_rf_nw
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_m2_rf_nw = 2
optimal_m2_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "avg_class_accuracy"])]
optimal_m2_rf_nw
n_trees = 500
m_values_w = seq(from = 2, to = 19, by = 1)
n_m_values_w = length(m_values_w)
rf_eval_metrics_w = setNames(data.frame(matrix(ncol = 4, nrow = n_m_values_w)),
c("m", "accuracy", "avg_class_accuracy", "f1"))
rf_eval_metrics_w[, "m"] = m_values_w
rf_eval_metrics_w = read.csv("forest_validation_results_w.csv")
plot(m_values_w, rf_eval_metrics_w[, "accuracy"], type = 'b', col = "black",
ylim = c(0.20, 0.40), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_w, rf_eval_metrics_w[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_rf_w = 0.3386939
max_val_accuracy_rf_w = max(rf_eval_metrics_w[, "accuracy"])
max_val_accuracy_rf_w
# max_val_avg_class_accuracy_rf_w = 0.2191808
max_val_avg_class_accuracy_rf_w = max(rf_eval_metrics_w[, "avg_class_accuracy"])
max_val_avg_class_accuracy_rf_w
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_w = 12
optimal_m1_rf_w = m_values_w[which.max(rf_eval_metrics_w[, "accuracy"])]
optimal_m1_rf_w
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_m2_rf_w = 2
optimal_m2_rf_w = m_values_w[which.max(rf_eval_metrics_w[, "avg_class_accuracy"])]
optimal_m2_rf_w
testing_accuracy1_lasso_nw
testing_avg_class_accuracy1_lasso_nw
o
# testing_f1_score1_lasso_nw = 0.1661076
testing_f1_score1_lasso_nw = comp0.1661076ute_eval_metrics(testing_yHat1_lasso_nw, testingY)[3]
testing_f1_score1_lasso_nw
# Results
# testing_accuracy2_lasso_nw =
testing_accuracy2_lasso_nw = compute_eval_metrics(testing_yHat2_lasso_nw, testingY)[1]
testing_accuracy2_lasso_nw
# Results
# testing_accuracy2_lasso_nw =
testing_accuracy2_lasso_nw = compute_eval_metrics(testing_yHat2_lasso_nw, testingY)[1]
## Applying the Optimal Lambda 2 to Test Dataset (Average Class Accuracy as Evaluation Metric)
best_fit2_lasso_nw = glmnet(final_trainingX, final_trainingY, family = 'multinomial', lambda = optimal_lambda2_lasso_nw)
testing_yHat2_lasso_nw = predict(best_fit2_lasso_nw, newx = testingX, type = "class")
testing_yHat2_lasso_nw = factor(testing_yHat2_lasso_nw, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat2_lasso_nw)
# Results
# testing_accuracy2_lasso_nw =
testing_accuracy2_lasso_nw = compute_eval_metrics(testing_yHat2_lasso_nw, testingY)[1]
testing_accuracy2_lasso_nw
# testing_avg_class_accuracy2_lasso_nw =
testing_avg_class_accuracy2_lasso_nw = compute_eval_metrics(testing_yHat2_lasso_nw, testingY)[2]
testing_avg_class_accuracy2_lasso_nw
# testing_f1_score2_lasso_nw =
testing_f1_score2_lasso_nw = compute_eval_metrics(testing_yHat2_lasso_nw, testingY)[3]
testing_f1_score2_lasso_nw
# Results
# testing_accuracy1_lasso_w = 0.3268462
testing_accuracy1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[1]
testing_accuracy1_lasso_w
# testing_avg_class_accuracy1_lasso_w = 0.2194199
testing_avg_class_accuracy1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[2]
testing_avg_class_accuracy1_lasso_w
# testing_f1_score1_lasso_w = 0.1618038
testing_f1_score1_lasso_w = compute_eval_metrics(testing_yHat1_lasso_w, testingY)[3]
testing_f1_score1_lasso_w
## Applying the Optimal Lambda 2 to Test Dataset (Average Class Accuracy as Evaluation Metric)
best_fit2_lasso_w = glmnet(final_trainingX, final_trainingY, family = 'multinomial',
lambda = optimal_lambda2_lasso_w, weights = final_training_weights)
testing_yHat2_lasso_w = predict(best_fit2_lasso_w, newx = testingX, type = "class")
testing_yHat2_lasso_w = factor(testing_yHat2_lasso_w, levels = levels(testingY))
# Confusion Matrix
table(testingY, testing_yHat2_lasso_w)
# Results
# testing_accuracy2_lasso_w = 0.2501999
testing_accuracy2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[1]
testing_accuracy2_lasso_w
# testing_avg_class_accuracy2_lasso_w = 0.238344
testing_avg_class_accuracy2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[2]
testing_avg_class_accuracy2_lasso_w
# testing_f1_score2_lasso_w = 0.1877949
testing_f1_score2_lasso_w = compute_eval_metrics(testing_yHat2_lasso_w, testingY)[3]
testing_f1_score2_lasso_w
n_trees = 500
m_values_nw = seq(from = 2, to = 19, by = 1)
n_m_values_nw = length(m_values_nw)
rf_eval_metrics_nw = setNames(data.frame(matrix(ncol = 4, nrow = n_m_values_nw)),
c("m", "accuracy", "avg_class_accuracy", "f1"))
rf_eval_metrics_nw[, "m"] = m_values_nw
rf_eval_metrics_nw = read.csv("forest_validation_results_nw.csv")
plot(m_values_nw, rf_eval_metrics_nw[, "accuracy"], type = 'b', col = "black",
ylim = c(0.15, 0.44), xlab = "m", ylab = "Evaluation Metric Value",
main = "m vs Evaluation Metric (Non - Weighted Random Forests)",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "avg_class_accuracy"], type = 'b', col = "red",
pch = 20, lwd = 0.5)
points(m_values_nw, rf_eval_metrics_nw[, "f1"], type = 'b', col = "blue",
pch = 20, lwd = 0.5)
legend(x = "topright", legend=c("Classification Accuracy", "Average Class Accuracy", "F1 - Score"),
col=c("black", "Red", "Blue"), lty=1, pch = c(16))
# max_val_accuracy_rf_nw = 0.4167037
max_val_accuracy_rf_nw = max(rf_eval_metrics_nw[, "accuracy"])
max_val_accuracy_rf_nw
# max_val_avg_class_accuracy_rf_nw = 0.1969875
max_val_avg_class_accuracy_rf_nw = max(rf_eval_metrics_nw[, "avg_class_accuracy"])
max_val_avg_class_accuracy_rf_nw
# Optimal m when Classification Accuracy is used as the Evaluation Metric
# optimal_m1_rf_nw = 2
optimal_m1_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "accuracy"])]
optimal_m1_rf_nw
# Optimal Lambda when Average Class Accuracy is used as the Evaluation Metric
# optimal_m2_rf_nw = 2
optimal_m2_rf_nw = m_values_nw[which.max(rf_eval_metrics_nw[, "avg_class_accuracy"])]
optimal_m2_rf_nw
optimal_m1_rf_nw
# Applying the Optimal m = 2 to Test Dataset (Accuracy as Metric)
best_fit_rf_nw = randomForest(outcome ~ ., data = final_training_data,
ntree = n_trees, mtry = optimal_m1_rf_nw, nodesize = 4)
gc()
# Applying the Optimal m = 2 to Test Dataset (Accuracy as Metric)
best_fit_rf_nw = randomForest(outcome ~ ., data = final_training_data,
ntree = n_trees, mtry = optimal_m1_rf_nw, nodesize = 4)
